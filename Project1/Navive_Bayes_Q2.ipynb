{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "665dbbc1",
   "metadata": {},
   "source": [
    "# Question: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "57fe4ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1.6530190426733' '72.871146648479' '24' 'W']\n",
      " ['1.6471384909498' '72.612785314988' '34' 'W']\n",
      " ['1.6472055785348' '73.53968351051' '33' 'M']\n",
      " ['1.7323008914951' '76.067870338779' '30' 'M']\n",
      " ['1.6750702657911' '81.05582111533' '30' 'M']\n",
      " ['1.5780970716644' '64.926084680188' '30' 'W']\n",
      " ['1.6587629355524' '69.38092449041' '30' 'M']\n",
      " ['1.6763295980234' '77.062295990149' '31' 'M']\n",
      " ['1.7187224085504' '62.112923317057' '37' 'W']\n",
      " ['1.5202218226439' '66.151444019603' '27' 'W']\n",
      " ['1.5552689261884' '66.076386143769' '31' 'W']\n",
      " ['1.6969333189258' '77.45386244568' '34' 'M']\n",
      " ['1.6887980792886' '76.489640732464' '37' 'M']\n",
      " ['1.5213552893624' '63.952944947832' '35' 'W']]\n",
      "[['1.62065758929' '59.376557437583' '32']\n",
      " ['1.7793983848363' '72.071775670801' '36']\n",
      " ['1.7004576585974' '66.267508112786' '31']\n",
      " ['1.6591086215159' '61.751621901787' '29']]\n"
     ]
    }
   ],
   "source": [
    "# Read and load the data\n",
    "import numpy as np\n",
    "\n",
    "def d_cleaning(line):\n",
    "    return line.replace('(', '').replace(')', '').replace(' ', '').strip().split(',')\n",
    "\n",
    "def d_fetching(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        d_input = f.readlines()\n",
    "        clean_input = list(map(d_cleaning, d_input))\n",
    "        f.close()\n",
    "    return clean_input\n",
    "\n",
    "def f_reading(dataset_path):\n",
    "    d_input = d_fetching(dataset_path)\n",
    "    input_np = np.array(d_input)\n",
    "    return input_np\n",
    "\n",
    "d_training = './dataset/1a_2a-training.txt'\n",
    "d_testing = './dataset/1a_2a-test.txt'\n",
    "d_large_120 = './dataset/1cd_2cd-data.txt'\n",
    "\n",
    "tp_train = f_reading(d_training)\n",
    "print(tp_train)\n",
    "tp_test = f_reading(d_testing)\n",
    "print(tp_test)\n",
    "tp_large = f_reading(d_large_120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "83bbe620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data\n",
    "train_X = []\n",
    "for i in range(tp_train.shape[0]):\n",
    "    train_X.append(np.array(tp_train[i][:-1], dtype=np.float32))\n",
    "train_X = np.array(train_X)\n",
    "\n",
    "test_X = np.array(tp_test, dtype=np.float32)\n",
    "    \n",
    "large_X = []\n",
    "for i in range(tp_large.shape[0]):\n",
    "    large_X.append(np.array(tp_large[i][:-1], dtype=np.float32))\n",
    "large_X = np.array(large_X)\n",
    "    \n",
    "train_Y = []\n",
    "for i in range(tp_train.shape[0]):\n",
    "    train_Y.append([tp_train[i][-1]])\n",
    "train_Y = np.array(train_Y, dtype=object)\n",
    "\n",
    "large_Y = []\n",
    "for i in range(tp_large.shape[0]):\n",
    "    large_Y.append([tp_large[i][-1]])\n",
    "large_Y = np.array(large_Y, dtype=object)\n",
    "\n",
    "data_train = np.concatenate((train_X, train_Y), axis=1)\n",
    "data_large = np.concatenate((large_X, large_Y), axis=1)\n",
    "data_test = np.concatenate((test_X, np.empty((test_X.shape[0],1), dtype=object)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be55edd3",
   "metadata": {},
   "source": [
    "# Part a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "2ff05239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting the data by classes\n",
    "def separating_class(data):\n",
    "    cs_d = dict()\n",
    "    for i in range(len(data)):\n",
    "        row = data[i]\n",
    "        cls_val = row[-1]\n",
    "        if cls_val not in cs_d:\n",
    "            cs_d[cls_val] = list()\n",
    "        cs_d[cls_val].append(row)\n",
    "    return cs_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "a4d8c44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the mean, stdev and count of each column\n",
    "def summary_d(data):\n",
    "    data = np.array(np.delete(data, len(data[0])-1, axis=1), dtype=np.float32)\n",
    "    temp = [(np.mean(col), np.std(col), len(col)) for col in zip(*data)]\n",
    "    return temp\n",
    "\n",
    "def summary_d_by_class(data):\n",
    "    cs_d = separating_class(data)\n",
    "    temp_data = dict()\n",
    "    for class_value, row in cs_d.items():\n",
    "        temp_data[class_value] = summary_d(row)\n",
    "    return temp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "ebcb9e93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Calculating the Gaussian Probability Distribution\n",
    "def cal_probablities(x, mean, stdev):\n",
    "    exp = math.exp(-((x-mean)**2 / (2 * stdev**2 )))\n",
    "    return (1 / (math.sqrt(2 * math.pi) * stdev)) * exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "91a073c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the probabilities of each class\n",
    "def calculate_class_probabilities(data, row):\n",
    "    tmp_summary_data = summary_d_by_class(data)\n",
    "    rows = sum([tmp_summary_data[i][0][2] for i in tmp_summary_data])\n",
    "    prob_val = dict()\n",
    "    for class_value, class_tmp_summary_data in tmp_summary_data.items():\n",
    "        prob_val[class_value] = tmp_summary_data[class_value][0][2]/float(rows)\n",
    "        for i in range(len(class_tmp_summary_data)):\n",
    "            mean, std, _ = class_tmp_summary_data[i]\n",
    "            prob_val[class_value] *= cal_probablities(row[i], mean, std)\n",
    "    return prob_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "7468d5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the class label with the highest probability\n",
    "def predict_nb(data, row):\n",
    "    prob_val = calculate_class_probabilities(data, row)\n",
    "    predict_val = max(zip(prob_val.values(), prob_val.keys()))[1]\n",
    "    return predict_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "24304c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For [1.6206575632095337 59.376556396484375 32.0] the predicted label is: W\n",
      "For [1.7793984413146973 72.07177734375 36.0] the predicted label is: W\n",
      "For [1.7004576921463013 66.26750946044922 31.0] the predicted label is: W\n",
      "For [1.6591086387634277 61.75162124633789 29.0] the predicted label is: W\n"
     ]
    }
   ],
   "source": [
    "# Prediction on the test data\n",
    "for i in range(len(data_test)):\n",
    "    print(f\"For {data_test[i][:-1]} the predicted label is: {predict_nb(data_train, data_test[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "23a8747b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the Leave One Out Evaluation function\n",
    "def leave_one_out_evaluation_nb(data):\n",
    "    predictions = []\n",
    "    for i in range(len(data)):\n",
    "        test_data = data[i]\n",
    "        train_data = np.delete(data, i, axis=0)\n",
    "        predictions.append(predict_nb(train_data, test_data))\n",
    "    predictions = np.array(predictions, dtype=object)\n",
    "    \n",
    "    temp = 0\n",
    "    for j in range(len(data)):\n",
    "        if data[j][-1] == predictions[j]:\n",
    "            temp+=1\n",
    "    \n",
    "    return temp/len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b140161",
   "metadata": {},
   "source": [
    "# Part c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "3a2663a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.0%\n"
     ]
    }
   ],
   "source": [
    "# Accuracy of the Gaussian Naive Bayes Model\n",
    "print(f\"Accuracy: {round(leave_one_out_evaluation_nb(data_large)*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d52da5",
   "metadata": {},
   "source": [
    "# Part d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "31f42991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.83%\n"
     ]
    }
   ],
   "source": [
    "# Accuracy of the Gaussian Naive Bayes Model after removing the age data\n",
    "n_large_data = np.delete(data_large, 2, axis=1)\n",
    "\n",
    "print(f\"Accuracy: {round(leave_one_out_evaluation_nb(n_large_data)*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb9c692",
   "metadata": {},
   "source": [
    "# Part e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ea85ca",
   "metadata": {},
   "source": [
    "By comparing the results of the two models discussed above, we can conclude that: \n",
    "In case 1, where all three features are taken into account, Gaussian Naive Bayes outperforms KNN.\n",
    "In case 2, with only two features, the previous two algorithms perform better overall, and in addition, Gaussian Naive Bayes and KNN (with K=3) perform similarly.\n",
    "We can therefore draw the conclusion that the Gaussian Naive Bayes algorithm performs better thanÂ KNN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
